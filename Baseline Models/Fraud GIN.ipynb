{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126c0694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "from functools import partial\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn import GINConv\n",
    "import numpy as np\n",
    "from pygod.utils import load_data as pygod_load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785209d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feat,\n",
    "        out_feat,\n",
    "        num_rels,\n",
    "        num_bases=-1,\n",
    "        bias=None,\n",
    "        activation=None,\n",
    "        is_input_layer=False,\n",
    "    ):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_feat = in_feat\n",
    "        self.out_feat = out_feat\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.is_input_layer = is_input_layer\n",
    "\n",
    "        # sanity check\n",
    "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
    "            self.num_bases = self.num_rels\n",
    "        # weight bases in equation (3)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.Tensor(self.num_bases, self.in_feat, self.out_feat)\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # linear combination coefficients in equation (3)\n",
    "            self.w_comp = nn.Parameter(\n",
    "                torch.Tensor(self.num_rels, self.num_bases)\n",
    "            )\n",
    "        # add bias\n",
    "        if self.bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
    "        # init trainable parameters\n",
    "        nn.init.xavier_uniform_(\n",
    "            self.weight, gain=nn.init.calculate_gain(\"relu\")\n",
    "        )\n",
    "        if self.num_bases < self.num_rels:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.w_comp, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "        if self.bias:\n",
    "            nn.init.xavier_uniform_(\n",
    "                self.bias, gain=nn.init.calculate_gain(\"relu\")\n",
    "            )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.num_bases < self.num_rels:\n",
    "            # generate all weights from bases (equation (3))\n",
    "            weight = self.weight.view(\n",
    "                self.in_feat, self.num_bases, self.out_feat\n",
    "            )\n",
    "            weight = torch.matmul(self.w_comp, weight).view(\n",
    "                self.num_rels, self.in_feat, self.out_feat\n",
    "            )\n",
    "        else:\n",
    "            weight = self.weight\n",
    "        if self.is_input_layer:\n",
    "\n",
    "            def message_func(edges):\n",
    "                # for input layer, matrix multiply can be converted to be\n",
    "                # an embedding lookup using source node id\n",
    "                embed = weight.view(-1, self.out_feat)\n",
    "                index = edges.data[dgl.ETYPE] * self.in_feat + edges.src[\"id\"]\n",
    "                return {\"msg\": embed[index] * edges.data[\"norm\"]}\n",
    "\n",
    "        else:\n",
    "\n",
    "            def message_func(edges):\n",
    "                w = weight[edges.data[dgl.ETYPE]]\n",
    "                msg = torch.bmm(edges.src[\"h\"].unsqueeze(1), w).squeeze()\n",
    "                msg = msg * edges.data[\"norm\"]\n",
    "                return {\"msg\": msg}\n",
    "\n",
    "        def apply_func(nodes):\n",
    "            h = nodes.data[\"h\"]\n",
    "            if self.bias:\n",
    "                h = h + self.bias\n",
    "            if self.activation:\n",
    "                h = self.activation(h)\n",
    "            return {\"h\": h}\n",
    "\n",
    "        g.update_all(message_func, fn.sum(msg=\"msg\", out=\"h\"), apply_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff0801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes,\n",
    "        h_dim,\n",
    "        out_dim,\n",
    "        num_rels,\n",
    "        num_bases=-1,\n",
    "        num_hidden_layers=1,\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.h_dim = h_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_rels = num_rels\n",
    "        self.num_bases = num_bases\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "\n",
    "        # create rgcn layers\n",
    "        self.build_model()\n",
    "\n",
    "        # create initial features\n",
    "        self.features = self.create_features()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.layers = nn.ModuleList()\n",
    "        # input to hidden\n",
    "        i2h = self.build_input_layer()\n",
    "        self.layers.append(i2h)\n",
    "        # hidden to hidden\n",
    "        for _ in range(self.num_hidden_layers):\n",
    "            h2h = self.build_hidden_layer()\n",
    "            self.layers.append(h2h)\n",
    "        # hidden to output\n",
    "        h2o = self.build_output_layer()\n",
    "        self.layers.append(h2o)\n",
    "\n",
    "    # initialize feature for each node\n",
    "    def create_features(self):\n",
    "        features = torch.arange(self.num_nodes)\n",
    "        return features\n",
    "\n",
    "    def build_input_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.num_nodes,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "            is_input_layer=True,\n",
    "        )\n",
    "\n",
    "    def build_hidden_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.h_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=F.relu,\n",
    "        )\n",
    "\n",
    "    def build_output_layer(self):\n",
    "        return RGCNLayer(\n",
    "            self.h_dim,\n",
    "            self.out_dim,\n",
    "            self.num_rels,\n",
    "            self.num_bases,\n",
    "            activation=partial(F.softmax, dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, g):\n",
    "        if self.features is not None:\n",
    "            g.ndata[\"id\"] = self.features\n",
    "        for layer in self.layers:\n",
    "            layer(g)\n",
    "        return g.ndata.pop(\"h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eccca5",
   "metadata": {},
   "source": [
    "# Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95abbf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import FraudAmazonDataset\n",
    "amazon = FraudAmazonDataset()\n",
    "g1 = amazon[0]\n",
    "num_classes = amazon.num_classes\n",
    "feat = g1.ndata['feature']\n",
    "label = g1.ndata['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8d8034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = g1.nodes['user'].data['train_mask']\n",
    "test_mask = g1.nodes['user'].data['test_mask']\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g1.nodes['user'].data.pop(\"label\")\n",
    "num_rels = len(g1.canonical_etypes)\n",
    "num_classes = amazon.num_classes\n",
    "\n",
    "# normalization factor\n",
    "for cetype in g1.canonical_etypes:\n",
    "    g1.edges[cetype].data[\"norm\"] = dgl.norm_by_dst(g1, cetype).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1b97afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = 16  # number of hidden units\n",
    "n_bases = -1  # use number of relations as number of bases\n",
    "n_hidden_layers = 0  # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 100  # epochs to train\n",
    "lr = 0.01  # learning rate\n",
    "l2norm = 0  # L2 norm coefficient\n",
    "\n",
    "# create graph\n",
    "g = dgl.to_homogeneous(g1, edata=[\"norm\"])\n",
    "node_ids = torch.arange(g.num_nodes())\n",
    "#target_idx = node_ids[g1.ndata[dgl.NTYPE] == category_id]\n",
    "\n",
    "# create model\n",
    "model = Model(\n",
    "    g.num_nodes(),\n",
    "    n_hidden,\n",
    "    num_classes,\n",
    "    num_rels,\n",
    "    num_bases=n_bases,\n",
    "    num_hidden_layers=n_hidden_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a7b493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.7037 | Train Loss: 0.6930 | Validation Accuracy: 0.6877 | Validation loss: 0.6930\n",
      "Epoch 00001 | Train Accuracy: 0.9056 | Train Loss: 0.6533 | Validation Accuracy: 0.8936 | Validation loss: 0.6552\n",
      "Epoch 00002 | Train Accuracy: 0.9056 | Train Loss: 0.6140 | Validation Accuracy: 0.8936 | Validation loss: 0.6181\n",
      "Epoch 00003 | Train Accuracy: 0.9057 | Train Loss: 0.5749 | Validation Accuracy: 0.8936 | Validation loss: 0.5812\n",
      "Epoch 00004 | Train Accuracy: 0.9057 | Train Loss: 0.5382 | Validation Accuracy: 0.8936 | Validation loss: 0.5466\n",
      "Epoch 00005 | Train Accuracy: 0.9054 | Train Loss: 0.5059 | Validation Accuracy: 0.8936 | Validation loss: 0.5160\n",
      "Epoch 00006 | Train Accuracy: 0.9054 | Train Loss: 0.4791 | Validation Accuracy: 0.8936 | Validation loss: 0.4905\n",
      "Epoch 00007 | Train Accuracy: 0.9054 | Train Loss: 0.4580 | Validation Accuracy: 0.8936 | Validation loss: 0.4703\n",
      "Epoch 00008 | Train Accuracy: 0.9054 | Train Loss: 0.4422 | Validation Accuracy: 0.8936 | Validation loss: 0.4552\n",
      "Epoch 00009 | Train Accuracy: 0.9054 | Train Loss: 0.4308 | Validation Accuracy: 0.8936 | Validation loss: 0.4442\n",
      "Epoch 00010 | Train Accuracy: 0.9054 | Train Loss: 0.4229 | Validation Accuracy: 0.8936 | Validation loss: 0.4365\n",
      "Epoch 00011 | Train Accuracy: 0.9054 | Train Loss: 0.4176 | Validation Accuracy: 0.8936 | Validation loss: 0.4312\n",
      "Epoch 00012 | Train Accuracy: 0.9054 | Train Loss: 0.4140 | Validation Accuracy: 0.8936 | Validation loss: 0.4276\n",
      "Epoch 00013 | Train Accuracy: 0.9054 | Train Loss: 0.4116 | Validation Accuracy: 0.8936 | Validation loss: 0.4252\n",
      "Epoch 00014 | Train Accuracy: 0.9054 | Train Loss: 0.4100 | Validation Accuracy: 0.8936 | Validation loss: 0.4235\n",
      "Epoch 00015 | Train Accuracy: 0.9054 | Train Loss: 0.4090 | Validation Accuracy: 0.8936 | Validation loss: 0.4224\n",
      "Epoch 00016 | Train Accuracy: 0.9054 | Train Loss: 0.4083 | Validation Accuracy: 0.8936 | Validation loss: 0.4216\n",
      "Epoch 00017 | Train Accuracy: 0.9054 | Train Loss: 0.4079 | Validation Accuracy: 0.8936 | Validation loss: 0.4211\n",
      "Epoch 00018 | Train Accuracy: 0.9054 | Train Loss: 0.4075 | Validation Accuracy: 0.8936 | Validation loss: 0.4207\n",
      "Epoch 00019 | Train Accuracy: 0.9054 | Train Loss: 0.4073 | Validation Accuracy: 0.8936 | Validation loss: 0.4204\n",
      "Epoch 00020 | Train Accuracy: 0.9054 | Train Loss: 0.4072 | Validation Accuracy: 0.8936 | Validation loss: 0.4202\n",
      "Epoch 00021 | Train Accuracy: 0.9054 | Train Loss: 0.4070 | Validation Accuracy: 0.8936 | Validation loss: 0.4201\n",
      "Epoch 00022 | Train Accuracy: 0.9057 | Train Loss: 0.4070 | Validation Accuracy: 0.8936 | Validation loss: 0.4200\n",
      "Epoch 00023 | Train Accuracy: 0.9057 | Train Loss: 0.4069 | Validation Accuracy: 0.8936 | Validation loss: 0.4199\n",
      "Epoch 00024 | Train Accuracy: 0.9057 | Train Loss: 0.4068 | Validation Accuracy: 0.8936 | Validation loss: 0.4198\n",
      "Epoch 00025 | Train Accuracy: 0.9059 | Train Loss: 0.4068 | Validation Accuracy: 0.8936 | Validation loss: 0.4198\n",
      "Epoch 00026 | Train Accuracy: 0.9059 | Train Loss: 0.4067 | Validation Accuracy: 0.8936 | Validation loss: 0.4198\n",
      "Epoch 00027 | Train Accuracy: 0.9059 | Train Loss: 0.4066 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00028 | Train Accuracy: 0.9059 | Train Loss: 0.4066 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00029 | Train Accuracy: 0.9059 | Train Loss: 0.4065 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00030 | Train Accuracy: 0.9059 | Train Loss: 0.4064 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00031 | Train Accuracy: 0.9059 | Train Loss: 0.4064 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00032 | Train Accuracy: 0.9061 | Train Loss: 0.4063 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00033 | Train Accuracy: 0.9062 | Train Loss: 0.4062 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00034 | Train Accuracy: 0.9062 | Train Loss: 0.4061 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00035 | Train Accuracy: 0.9066 | Train Loss: 0.4060 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00036 | Train Accuracy: 0.9067 | Train Loss: 0.4059 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00037 | Train Accuracy: 0.9067 | Train Loss: 0.4058 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00038 | Train Accuracy: 0.9067 | Train Loss: 0.4057 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00039 | Train Accuracy: 0.9067 | Train Loss: 0.4056 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00040 | Train Accuracy: 0.9069 | Train Loss: 0.4055 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00041 | Train Accuracy: 0.9069 | Train Loss: 0.4053 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00042 | Train Accuracy: 0.9071 | Train Loss: 0.4052 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00043 | Train Accuracy: 0.9072 | Train Loss: 0.4051 | Validation Accuracy: 0.8936 | Validation loss: 0.4197\n",
      "Epoch 00044 | Train Accuracy: 0.9072 | Train Loss: 0.4049 | Validation Accuracy: 0.8936 | Validation loss: 0.4198\n",
      "Epoch 00045 | Train Accuracy: 0.9076 | Train Loss: 0.4048 | Validation Accuracy: 0.8936 | Validation loss: 0.4198\n",
      "Epoch 00046 | Train Accuracy: 0.9076 | Train Loss: 0.4047 | Validation Accuracy: 0.8930 | Validation loss: 0.4198\n",
      "Epoch 00047 | Train Accuracy: 0.9079 | Train Loss: 0.4045 | Validation Accuracy: 0.8930 | Validation loss: 0.4198\n",
      "Epoch 00048 | Train Accuracy: 0.9079 | Train Loss: 0.4043 | Validation Accuracy: 0.8930 | Validation loss: 0.4199\n",
      "Epoch 00049 | Train Accuracy: 0.9079 | Train Loss: 0.4042 | Validation Accuracy: 0.8930 | Validation loss: 0.4199\n",
      "Epoch 00050 | Train Accuracy: 0.9082 | Train Loss: 0.4040 | Validation Accuracy: 0.8930 | Validation loss: 0.4199\n",
      "Epoch 00051 | Train Accuracy: 0.9082 | Train Loss: 0.4038 | Validation Accuracy: 0.8930 | Validation loss: 0.4199\n",
      "Epoch 00052 | Train Accuracy: 0.9084 | Train Loss: 0.4036 | Validation Accuracy: 0.8930 | Validation loss: 0.4199\n",
      "Epoch 00053 | Train Accuracy: 0.9084 | Train Loss: 0.4034 | Validation Accuracy: 0.8930 | Validation loss: 0.4200\n",
      "Epoch 00054 | Train Accuracy: 0.9085 | Train Loss: 0.4031 | Validation Accuracy: 0.8930 | Validation loss: 0.4200\n",
      "Epoch 00055 | Train Accuracy: 0.9090 | Train Loss: 0.4029 | Validation Accuracy: 0.8930 | Validation loss: 0.4200\n",
      "Epoch 00056 | Train Accuracy: 0.9092 | Train Loss: 0.4026 | Validation Accuracy: 0.8930 | Validation loss: 0.4200\n",
      "Epoch 00057 | Train Accuracy: 0.9094 | Train Loss: 0.4024 | Validation Accuracy: 0.8924 | Validation loss: 0.4200\n",
      "Epoch 00058 | Train Accuracy: 0.9097 | Train Loss: 0.4021 | Validation Accuracy: 0.8930 | Validation loss: 0.4201\n",
      "Epoch 00059 | Train Accuracy: 0.9100 | Train Loss: 0.4018 | Validation Accuracy: 0.8930 | Validation loss: 0.4201\n",
      "Epoch 00060 | Train Accuracy: 0.9105 | Train Loss: 0.4015 | Validation Accuracy: 0.8924 | Validation loss: 0.4201\n",
      "Epoch 00061 | Train Accuracy: 0.9107 | Train Loss: 0.4011 | Validation Accuracy: 0.8924 | Validation loss: 0.4201\n",
      "Epoch 00062 | Train Accuracy: 0.9112 | Train Loss: 0.4008 | Validation Accuracy: 0.8918 | Validation loss: 0.4200\n",
      "Epoch 00063 | Train Accuracy: 0.9119 | Train Loss: 0.4004 | Validation Accuracy: 0.8924 | Validation loss: 0.4200\n",
      "Epoch 00064 | Train Accuracy: 0.9120 | Train Loss: 0.4000 | Validation Accuracy: 0.8924 | Validation loss: 0.4200\n",
      "Epoch 00065 | Train Accuracy: 0.9130 | Train Loss: 0.3997 | Validation Accuracy: 0.8924 | Validation loss: 0.4199\n",
      "Epoch 00066 | Train Accuracy: 0.9130 | Train Loss: 0.3993 | Validation Accuracy: 0.8930 | Validation loss: 0.4198\n",
      "Epoch 00067 | Train Accuracy: 0.9130 | Train Loss: 0.3989 | Validation Accuracy: 0.8930 | Validation loss: 0.4197\n",
      "Epoch 00068 | Train Accuracy: 0.9130 | Train Loss: 0.3984 | Validation Accuracy: 0.8924 | Validation loss: 0.4197\n",
      "Epoch 00069 | Train Accuracy: 0.9137 | Train Loss: 0.3980 | Validation Accuracy: 0.8924 | Validation loss: 0.4196\n",
      "Epoch 00070 | Train Accuracy: 0.9137 | Train Loss: 0.3975 | Validation Accuracy: 0.8924 | Validation loss: 0.4195\n",
      "Epoch 00071 | Train Accuracy: 0.9140 | Train Loss: 0.3969 | Validation Accuracy: 0.8930 | Validation loss: 0.4193\n",
      "Epoch 00072 | Train Accuracy: 0.9142 | Train Loss: 0.3963 | Validation Accuracy: 0.8930 | Validation loss: 0.4192\n",
      "Epoch 00073 | Train Accuracy: 0.9147 | Train Loss: 0.3956 | Validation Accuracy: 0.8924 | Validation loss: 0.4191\n",
      "Epoch 00074 | Train Accuracy: 0.9157 | Train Loss: 0.3949 | Validation Accuracy: 0.8930 | Validation loss: 0.4189\n",
      "Epoch 00075 | Train Accuracy: 0.9160 | Train Loss: 0.3941 | Validation Accuracy: 0.8942 | Validation loss: 0.4187\n",
      "Epoch 00076 | Train Accuracy: 0.9180 | Train Loss: 0.3932 | Validation Accuracy: 0.8953 | Validation loss: 0.4185\n",
      "Epoch 00077 | Train Accuracy: 0.9188 | Train Loss: 0.3922 | Validation Accuracy: 0.8936 | Validation loss: 0.4182\n",
      "Epoch 00078 | Train Accuracy: 0.9195 | Train Loss: 0.3912 | Validation Accuracy: 0.8936 | Validation loss: 0.4180\n",
      "Epoch 00079 | Train Accuracy: 0.9205 | Train Loss: 0.3901 | Validation Accuracy: 0.8936 | Validation loss: 0.4177\n",
      "Epoch 00080 | Train Accuracy: 0.9221 | Train Loss: 0.3888 | Validation Accuracy: 0.8930 | Validation loss: 0.4173\n",
      "Epoch 00081 | Train Accuracy: 0.9236 | Train Loss: 0.3875 | Validation Accuracy: 0.8942 | Validation loss: 0.4168\n",
      "Epoch 00082 | Train Accuracy: 0.9251 | Train Loss: 0.3862 | Validation Accuracy: 0.8942 | Validation loss: 0.4163\n",
      "Epoch 00083 | Train Accuracy: 0.9276 | Train Loss: 0.3848 | Validation Accuracy: 0.8947 | Validation loss: 0.4157\n",
      "Epoch 00084 | Train Accuracy: 0.9287 | Train Loss: 0.3834 | Validation Accuracy: 0.8953 | Validation loss: 0.4151\n",
      "Epoch 00085 | Train Accuracy: 0.9312 | Train Loss: 0.3821 | Validation Accuracy: 0.8965 | Validation loss: 0.4146\n",
      "Epoch 00086 | Train Accuracy: 0.9327 | Train Loss: 0.3809 | Validation Accuracy: 0.8976 | Validation loss: 0.4142\n",
      "Epoch 00087 | Train Accuracy: 0.9343 | Train Loss: 0.3797 | Validation Accuracy: 0.8976 | Validation loss: 0.4139\n",
      "Epoch 00088 | Train Accuracy: 0.9370 | Train Loss: 0.3786 | Validation Accuracy: 0.8976 | Validation loss: 0.4136\n",
      "Epoch 00089 | Train Accuracy: 0.9405 | Train Loss: 0.3773 | Validation Accuracy: 0.8994 | Validation loss: 0.4132\n",
      "Epoch 00090 | Train Accuracy: 0.9413 | Train Loss: 0.3758 | Validation Accuracy: 0.9005 | Validation loss: 0.4126\n",
      "Epoch 00091 | Train Accuracy: 0.9426 | Train Loss: 0.3742 | Validation Accuracy: 0.9023 | Validation loss: 0.4118\n",
      "Epoch 00092 | Train Accuracy: 0.9438 | Train Loss: 0.3725 | Validation Accuracy: 0.9017 | Validation loss: 0.4111\n",
      "Epoch 00093 | Train Accuracy: 0.9453 | Train Loss: 0.3709 | Validation Accuracy: 0.9028 | Validation loss: 0.4104\n",
      "Epoch 00094 | Train Accuracy: 0.9464 | Train Loss: 0.3694 | Validation Accuracy: 0.9034 | Validation loss: 0.4097\n",
      "Epoch 00095 | Train Accuracy: 0.9481 | Train Loss: 0.3680 | Validation Accuracy: 0.9034 | Validation loss: 0.4091\n",
      "Epoch 00096 | Train Accuracy: 0.9494 | Train Loss: 0.3666 | Validation Accuracy: 0.9046 | Validation loss: 0.4085\n",
      "Epoch 00097 | Train Accuracy: 0.9511 | Train Loss: 0.3652 | Validation Accuracy: 0.9046 | Validation loss: 0.4079\n",
      "Epoch 00098 | Train Accuracy: 0.9534 | Train Loss: 0.3639 | Validation Accuracy: 0.9063 | Validation loss: 0.4073\n",
      "Epoch 00099 | Train Accuracy: 0.9542 | Train Loss: 0.3626 | Validation Accuracy: 0.9069 | Validation loss: 0.4067\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    #logits = logits[target_idx]\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    val_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx])\n",
    "    val_acc = val_acc.item() / len(test_idx)\n",
    "    print(\n",
    "        \"Epoch {:05d} | \".format(epoch)\n",
    "        + \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "            train_acc, loss.item()\n",
    "        )\n",
    "        + \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "            val_acc, val_loss.item()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf5eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8294357675531168\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# After training, when evaluating on test data:\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(g)\n",
    "    probabilities = F.softmax(logits[test_idx], dim=1)  # Convert logits to probabilities\n",
    "    # Assuming your positive class is 1 (adjust accordingly if it's different)\n",
    "    positive_probabilities = probabilities[:, 1]  # Get probabilities for the positive class\n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc = roc_auc_score(labels[test_idx].cpu(), positive_probabilities.cpu())\n",
    "    print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d906fd",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc7f5a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import FraudYelpDataset\n",
    "yelp = FraudYelpDataset()\n",
    "g2 = yelp[0]\n",
    "num_classes = yelp.num_classes\n",
    "feat = g2.ndata['feature']\n",
    "label = g2.ndata['label']\n",
    "train_mask = g2.ndata['train_mask']\n",
    "test_mask = g2.ndata['test_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06116eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = g2.nodes['review'].data['train_mask']\n",
    "test_mask = g2.nodes['review'].data['test_mask']\n",
    "train_idx = torch.nonzero(train_mask, as_tuple=False).squeeze()\n",
    "test_idx = torch.nonzero(test_mask, as_tuple=False).squeeze()\n",
    "labels = g2.nodes['review'].data.pop(\"label\")\n",
    "num_rels = len(g2.canonical_etypes)\n",
    "num_classes = amazon.num_classes\n",
    "\n",
    "# normalization factor\n",
    "for cetype in g2.canonical_etypes:\n",
    "    g2.edges[cetype].data[\"norm\"] = dgl.norm_by_dst(g2, cetype).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fdc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = 16  # number of hidden units\n",
    "n_bases = -1  # use number of relations as number of bases\n",
    "n_hidden_layers = 0  # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 100  # epochs to train\n",
    "lr = 0.01  # learning rate\n",
    "l2norm = 0  # L2 norm coefficient\n",
    "\n",
    "# create graph\n",
    "g = dgl.to_homogeneous(g2, edata=[\"norm\"])\n",
    "node_ids = torch.arange(g.num_nodes())\n",
    "#target_idx = node_ids[g2.ndata[dgl.NTYPE] == category_id]\n",
    "\n",
    "# create model\n",
    "model = Model(\n",
    "    g.num_nodes(),\n",
    "    n_hidden,\n",
    "    num_classes,\n",
    "    num_rels,\n",
    "    num_bases=n_bases,\n",
    "    num_hidden_layers=n_hidden_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50db23c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "Epoch 00000 | Train Accuracy: 0.7030 | Train Loss: 0.6929 | Validation Accuracy: 0.7103 | Validation loss: 0.6929\n",
      "Epoch 00001 | Train Accuracy: 0.8586 | Train Loss: 0.6646 | Validation Accuracy: 0.8610 | Validation loss: 0.6671\n",
      "Epoch 00002 | Train Accuracy: 0.8586 | Train Loss: 0.6335 | Validation Accuracy: 0.8609 | Validation loss: 0.6394\n",
      "Epoch 00003 | Train Accuracy: 0.8586 | Train Loss: 0.6016 | Validation Accuracy: 0.8607 | Validation loss: 0.6103\n",
      "Epoch 00004 | Train Accuracy: 0.8586 | Train Loss: 0.5708 | Validation Accuracy: 0.8609 | Validation loss: 0.5813\n",
      "Epoch 00005 | Train Accuracy: 0.8584 | Train Loss: 0.5430 | Validation Accuracy: 0.8610 | Validation loss: 0.5540\n",
      "Epoch 00006 | Train Accuracy: 0.8583 | Train Loss: 0.5192 | Validation Accuracy: 0.8610 | Validation loss: 0.5297\n",
      "Epoch 00007 | Train Accuracy: 0.8583 | Train Loss: 0.5001 | Validation Accuracy: 0.8610 | Validation loss: 0.5092\n",
      "Epoch 00008 | Train Accuracy: 0.8583 | Train Loss: 0.4855 | Validation Accuracy: 0.8610 | Validation loss: 0.4930\n",
      "Epoch 00009 | Train Accuracy: 0.8583 | Train Loss: 0.4750 | Validation Accuracy: 0.8610 | Validation loss: 0.4808\n",
      "Epoch 00010 | Train Accuracy: 0.8584 | Train Loss: 0.4676 | Validation Accuracy: 0.8610 | Validation loss: 0.4719\n",
      "Epoch 00011 | Train Accuracy: 0.8584 | Train Loss: 0.4626 | Validation Accuracy: 0.8610 | Validation loss: 0.4656\n",
      "Epoch 00012 | Train Accuracy: 0.8586 | Train Loss: 0.4593 | Validation Accuracy: 0.8613 | Validation loss: 0.4612\n",
      "Epoch 00013 | Train Accuracy: 0.8587 | Train Loss: 0.4570 | Validation Accuracy: 0.8613 | Validation loss: 0.4582\n",
      "Epoch 00014 | Train Accuracy: 0.8586 | Train Loss: 0.4554 | Validation Accuracy: 0.8613 | Validation loss: 0.4561\n",
      "Epoch 00015 | Train Accuracy: 0.8589 | Train Loss: 0.4543 | Validation Accuracy: 0.8614 | Validation loss: 0.4546\n",
      "Epoch 00016 | Train Accuracy: 0.8592 | Train Loss: 0.4535 | Validation Accuracy: 0.8614 | Validation loss: 0.4535\n",
      "Epoch 00017 | Train Accuracy: 0.8594 | Train Loss: 0.4529 | Validation Accuracy: 0.8615 | Validation loss: 0.4527\n",
      "Epoch 00018 | Train Accuracy: 0.8596 | Train Loss: 0.4524 | Validation Accuracy: 0.8615 | Validation loss: 0.4522\n",
      "Epoch 00019 | Train Accuracy: 0.8597 | Train Loss: 0.4519 | Validation Accuracy: 0.8617 | Validation loss: 0.4517\n",
      "Epoch 00020 | Train Accuracy: 0.8603 | Train Loss: 0.4515 | Validation Accuracy: 0.8621 | Validation loss: 0.4514\n",
      "Epoch 00021 | Train Accuracy: 0.8606 | Train Loss: 0.4512 | Validation Accuracy: 0.8622 | Validation loss: 0.4511\n",
      "Epoch 00022 | Train Accuracy: 0.8610 | Train Loss: 0.4508 | Validation Accuracy: 0.8627 | Validation loss: 0.4509\n",
      "Epoch 00023 | Train Accuracy: 0.8613 | Train Loss: 0.4505 | Validation Accuracy: 0.8623 | Validation loss: 0.4507\n",
      "Epoch 00024 | Train Accuracy: 0.8615 | Train Loss: 0.4502 | Validation Accuracy: 0.8625 | Validation loss: 0.4506\n",
      "Epoch 00025 | Train Accuracy: 0.8618 | Train Loss: 0.4498 | Validation Accuracy: 0.8625 | Validation loss: 0.4505\n",
      "Epoch 00026 | Train Accuracy: 0.8621 | Train Loss: 0.4495 | Validation Accuracy: 0.8626 | Validation loss: 0.4504\n",
      "Epoch 00027 | Train Accuracy: 0.8623 | Train Loss: 0.4492 | Validation Accuracy: 0.8626 | Validation loss: 0.4504\n",
      "Epoch 00028 | Train Accuracy: 0.8626 | Train Loss: 0.4489 | Validation Accuracy: 0.8626 | Validation loss: 0.4503\n",
      "Epoch 00029 | Train Accuracy: 0.8627 | Train Loss: 0.4486 | Validation Accuracy: 0.8626 | Validation loss: 0.4503\n",
      "Epoch 00030 | Train Accuracy: 0.8632 | Train Loss: 0.4482 | Validation Accuracy: 0.8624 | Validation loss: 0.4503\n",
      "Epoch 00031 | Train Accuracy: 0.8634 | Train Loss: 0.4479 | Validation Accuracy: 0.8621 | Validation loss: 0.4503\n",
      "Epoch 00032 | Train Accuracy: 0.8633 | Train Loss: 0.4475 | Validation Accuracy: 0.8617 | Validation loss: 0.4503\n",
      "Epoch 00033 | Train Accuracy: 0.8635 | Train Loss: 0.4471 | Validation Accuracy: 0.8618 | Validation loss: 0.4503\n",
      "Epoch 00034 | Train Accuracy: 0.8638 | Train Loss: 0.4467 | Validation Accuracy: 0.8617 | Validation loss: 0.4502\n",
      "Epoch 00035 | Train Accuracy: 0.8642 | Train Loss: 0.4463 | Validation Accuracy: 0.8619 | Validation loss: 0.4502\n",
      "Epoch 00036 | Train Accuracy: 0.8647 | Train Loss: 0.4459 | Validation Accuracy: 0.8619 | Validation loss: 0.4502\n",
      "Epoch 00037 | Train Accuracy: 0.8656 | Train Loss: 0.4454 | Validation Accuracy: 0.8623 | Validation loss: 0.4501\n",
      "Epoch 00038 | Train Accuracy: 0.8663 | Train Loss: 0.4450 | Validation Accuracy: 0.8624 | Validation loss: 0.4501\n",
      "Epoch 00039 | Train Accuracy: 0.8668 | Train Loss: 0.4445 | Validation Accuracy: 0.8627 | Validation loss: 0.4501\n",
      "Epoch 00040 | Train Accuracy: 0.8673 | Train Loss: 0.4440 | Validation Accuracy: 0.8630 | Validation loss: 0.4501\n",
      "Epoch 00041 | Train Accuracy: 0.8680 | Train Loss: 0.4435 | Validation Accuracy: 0.8631 | Validation loss: 0.4502\n",
      "Epoch 00042 | Train Accuracy: 0.8684 | Train Loss: 0.4430 | Validation Accuracy: 0.8631 | Validation loss: 0.4503\n",
      "Epoch 00043 | Train Accuracy: 0.8693 | Train Loss: 0.4425 | Validation Accuracy: 0.8628 | Validation loss: 0.4504\n",
      "Epoch 00044 | Train Accuracy: 0.8698 | Train Loss: 0.4420 | Validation Accuracy: 0.8617 | Validation loss: 0.4505\n",
      "Epoch 00045 | Train Accuracy: 0.8704 | Train Loss: 0.4415 | Validation Accuracy: 0.8610 | Validation loss: 0.4507\n",
      "Epoch 00046 | Train Accuracy: 0.8710 | Train Loss: 0.4410 | Validation Accuracy: 0.8606 | Validation loss: 0.4509\n",
      "Epoch 00047 | Train Accuracy: 0.8713 | Train Loss: 0.4405 | Validation Accuracy: 0.8602 | Validation loss: 0.4512\n",
      "Epoch 00048 | Train Accuracy: 0.8719 | Train Loss: 0.4400 | Validation Accuracy: 0.8603 | Validation loss: 0.4515\n",
      "Epoch 00049 | Train Accuracy: 0.8725 | Train Loss: 0.4395 | Validation Accuracy: 0.8601 | Validation loss: 0.4518\n",
      "Epoch 00050 | Train Accuracy: 0.8730 | Train Loss: 0.4390 | Validation Accuracy: 0.8601 | Validation loss: 0.4521\n",
      "Epoch 00051 | Train Accuracy: 0.8734 | Train Loss: 0.4384 | Validation Accuracy: 0.8600 | Validation loss: 0.4524\n",
      "Epoch 00052 | Train Accuracy: 0.8742 | Train Loss: 0.4379 | Validation Accuracy: 0.8598 | Validation loss: 0.4527\n",
      "Epoch 00053 | Train Accuracy: 0.8749 | Train Loss: 0.4373 | Validation Accuracy: 0.8598 | Validation loss: 0.4530\n",
      "Epoch 00054 | Train Accuracy: 0.8755 | Train Loss: 0.4367 | Validation Accuracy: 0.8598 | Validation loss: 0.4533\n",
      "Epoch 00055 | Train Accuracy: 0.8762 | Train Loss: 0.4361 | Validation Accuracy: 0.8592 | Validation loss: 0.4536\n",
      "Epoch 00056 | Train Accuracy: 0.8770 | Train Loss: 0.4355 | Validation Accuracy: 0.8593 | Validation loss: 0.4540\n",
      "Epoch 00057 | Train Accuracy: 0.8778 | Train Loss: 0.4348 | Validation Accuracy: 0.8589 | Validation loss: 0.4543\n",
      "Epoch 00058 | Train Accuracy: 0.8782 | Train Loss: 0.4341 | Validation Accuracy: 0.8579 | Validation loss: 0.4546\n",
      "Epoch 00059 | Train Accuracy: 0.8791 | Train Loss: 0.4334 | Validation Accuracy: 0.8573 | Validation loss: 0.4550\n",
      "Epoch 00060 | Train Accuracy: 0.8797 | Train Loss: 0.4327 | Validation Accuracy: 0.8572 | Validation loss: 0.4553\n",
      "Epoch 00061 | Train Accuracy: 0.8806 | Train Loss: 0.4320 | Validation Accuracy: 0.8556 | Validation loss: 0.4557\n",
      "Epoch 00062 | Train Accuracy: 0.8816 | Train Loss: 0.4312 | Validation Accuracy: 0.8559 | Validation loss: 0.4560\n",
      "Epoch 00063 | Train Accuracy: 0.8825 | Train Loss: 0.4304 | Validation Accuracy: 0.8554 | Validation loss: 0.4564\n",
      "Epoch 00064 | Train Accuracy: 0.8833 | Train Loss: 0.4296 | Validation Accuracy: 0.8541 | Validation loss: 0.4568\n",
      "Epoch 00065 | Train Accuracy: 0.8841 | Train Loss: 0.4288 | Validation Accuracy: 0.8535 | Validation loss: 0.4572\n",
      "Epoch 00066 | Train Accuracy: 0.8853 | Train Loss: 0.4280 | Validation Accuracy: 0.8528 | Validation loss: 0.4575\n",
      "Epoch 00067 | Train Accuracy: 0.8865 | Train Loss: 0.4271 | Validation Accuracy: 0.8526 | Validation loss: 0.4579\n",
      "Epoch 00068 | Train Accuracy: 0.8878 | Train Loss: 0.4262 | Validation Accuracy: 0.8525 | Validation loss: 0.4583\n",
      "Epoch 00069 | Train Accuracy: 0.8890 | Train Loss: 0.4253 | Validation Accuracy: 0.8524 | Validation loss: 0.4588\n",
      "Epoch 00070 | Train Accuracy: 0.8899 | Train Loss: 0.4244 | Validation Accuracy: 0.8527 | Validation loss: 0.4592\n",
      "Epoch 00071 | Train Accuracy: 0.8914 | Train Loss: 0.4235 | Validation Accuracy: 0.8523 | Validation loss: 0.4596\n",
      "Epoch 00072 | Train Accuracy: 0.8922 | Train Loss: 0.4225 | Validation Accuracy: 0.8520 | Validation loss: 0.4600\n",
      "Epoch 00073 | Train Accuracy: 0.8937 | Train Loss: 0.4216 | Validation Accuracy: 0.8518 | Validation loss: 0.4603\n",
      "Epoch 00074 | Train Accuracy: 0.8950 | Train Loss: 0.4206 | Validation Accuracy: 0.8518 | Validation loss: 0.4607\n",
      "Epoch 00075 | Train Accuracy: 0.8957 | Train Loss: 0.4197 | Validation Accuracy: 0.8524 | Validation loss: 0.4610\n",
      "Epoch 00076 | Train Accuracy: 0.8969 | Train Loss: 0.4187 | Validation Accuracy: 0.8523 | Validation loss: 0.4612\n",
      "Epoch 00077 | Train Accuracy: 0.8983 | Train Loss: 0.4178 | Validation Accuracy: 0.8522 | Validation loss: 0.4614\n",
      "Epoch 00078 | Train Accuracy: 0.8992 | Train Loss: 0.4168 | Validation Accuracy: 0.8518 | Validation loss: 0.4615\n",
      "Epoch 00079 | Train Accuracy: 0.9000 | Train Loss: 0.4159 | Validation Accuracy: 0.8523 | Validation loss: 0.4617\n",
      "Epoch 00080 | Train Accuracy: 0.9011 | Train Loss: 0.4149 | Validation Accuracy: 0.8522 | Validation loss: 0.4617\n",
      "Epoch 00081 | Train Accuracy: 0.9020 | Train Loss: 0.4140 | Validation Accuracy: 0.8525 | Validation loss: 0.4618\n",
      "Epoch 00082 | Train Accuracy: 0.9030 | Train Loss: 0.4130 | Validation Accuracy: 0.8525 | Validation loss: 0.4619\n",
      "Epoch 00083 | Train Accuracy: 0.9044 | Train Loss: 0.4121 | Validation Accuracy: 0.8529 | Validation loss: 0.4619\n",
      "Epoch 00084 | Train Accuracy: 0.9054 | Train Loss: 0.4112 | Validation Accuracy: 0.8531 | Validation loss: 0.4621\n",
      "Epoch 00085 | Train Accuracy: 0.9064 | Train Loss: 0.4103 | Validation Accuracy: 0.8532 | Validation loss: 0.4622\n",
      "Epoch 00086 | Train Accuracy: 0.9077 | Train Loss: 0.4094 | Validation Accuracy: 0.8529 | Validation loss: 0.4624\n",
      "Epoch 00087 | Train Accuracy: 0.9084 | Train Loss: 0.4086 | Validation Accuracy: 0.8519 | Validation loss: 0.4626\n",
      "Epoch 00088 | Train Accuracy: 0.9094 | Train Loss: 0.4078 | Validation Accuracy: 0.8517 | Validation loss: 0.4629\n",
      "Epoch 00089 | Train Accuracy: 0.9101 | Train Loss: 0.4070 | Validation Accuracy: 0.8512 | Validation loss: 0.4631\n",
      "Epoch 00090 | Train Accuracy: 0.9112 | Train Loss: 0.4063 | Validation Accuracy: 0.8502 | Validation loss: 0.4634\n",
      "Epoch 00091 | Train Accuracy: 0.9121 | Train Loss: 0.4055 | Validation Accuracy: 0.8503 | Validation loss: 0.4637\n",
      "Epoch 00092 | Train Accuracy: 0.9130 | Train Loss: 0.4048 | Validation Accuracy: 0.8501 | Validation loss: 0.4639\n",
      "Epoch 00093 | Train Accuracy: 0.9138 | Train Loss: 0.4042 | Validation Accuracy: 0.8497 | Validation loss: 0.4641\n",
      "Epoch 00094 | Train Accuracy: 0.9143 | Train Loss: 0.4035 | Validation Accuracy: 0.8491 | Validation loss: 0.4643\n",
      "Epoch 00095 | Train Accuracy: 0.9149 | Train Loss: 0.4028 | Validation Accuracy: 0.8489 | Validation loss: 0.4644\n",
      "Epoch 00096 | Train Accuracy: 0.9155 | Train Loss: 0.4022 | Validation Accuracy: 0.8478 | Validation loss: 0.4645\n",
      "Epoch 00097 | Train Accuracy: 0.9161 | Train Loss: 0.4016 | Validation Accuracy: 0.8478 | Validation loss: 0.4646\n",
      "Epoch 00098 | Train Accuracy: 0.9165 | Train Loss: 0.4010 | Validation Accuracy: 0.8475 | Validation loss: 0.4647\n",
      "Epoch 00099 | Train Accuracy: 0.9169 | Train Loss: 0.4004 | Validation Accuracy: 0.8470 | Validation loss: 0.4648\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2norm)\n",
    "\n",
    "print(\"start training...\")\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model.forward(g)\n",
    "    #logits = logits[target_idx]\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = torch.sum(logits[train_idx].argmax(dim=1) == labels[train_idx])\n",
    "    train_acc = train_acc.item() / len(train_idx)\n",
    "    val_loss = F.cross_entropy(logits[test_idx], labels[test_idx])\n",
    "    val_acc = torch.sum(logits[test_idx].argmax(dim=1) == labels[test_idx])\n",
    "    val_acc = val_acc.item() / len(test_idx)\n",
    "    print(\n",
    "        \"Epoch {:05d} | \".format(epoch)\n",
    "        + \"Train Accuracy: {:.4f} | Train Loss: {:.4f} | \".format(\n",
    "            train_acc, loss.item()\n",
    "        )\n",
    "        + \"Validation Accuracy: {:.4f} | Validation loss: {:.4f}\".format(\n",
    "            val_acc, val_loss.item()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d59157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.7329092264805646\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# After training, when evaluating on test data:\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(g)\n",
    "    probabilities = F.softmax(logits[test_idx], dim=1)  # Convert logits to probabilities\n",
    "    # Assuming your positive class is 1 (adjust accordingly if it's different)\n",
    "    positive_probabilities = probabilities[:, 1]  # Get probabilities for the positive class\n",
    "    # Calculate ROC-AUC\n",
    "    roc_auc = roc_auc_score(labels[test_idx].cpu(), positive_probabilities.cpu())\n",
    "    print(f\"ROC-AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9a1d6",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23da815",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pygod_load_data('reddit')\n",
    "g3 = dgl.graph((reddit.edge_index[0], reddit.edge_index[1]))\n",
    "g3.ndata['feature'] = reddit.x\n",
    "g3.ndata['label'] = reddit.y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4a47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = g3.number_of_nodes()\n",
    "indices = np.random.permutation(num_nodes)\n",
    "\n",
    "# Assuming 70% training, 15% validation, 15% testing split\n",
    "train_size = int(num_nodes * 0.8)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[indices[:train_size]] = True\n",
    "test_mask[indices[train_size:]] = True\n",
    "\n",
    "# Assign the masks to your graph\n",
    "g3.ndata['train_mask'] = train_mask\n",
    "g3.ndata['test_mask'] = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a9f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"\n",
    "    This module applies a linear transformation followed by a non-linearity.\n",
    "    \"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = F.relu(h)\n",
    "        return h\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GINLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.apply_func = ApplyNodeFunc(self.linear)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        ginconv = GINConv(self.apply_func, 'sum')  # 'sum' is the aggregator type\n",
    "        return ginconv(g, h)\n",
    "\n",
    "class GINModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_layers):\n",
    "        super(GINModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Input layer\n",
    "        self.layers.append(GINLayer(in_feats, hidden_feats))\n",
    "        # Hidden layers\n",
    "        for i in range(num_layers - 2):\n",
    "            self.layers.append(GINLayer(hidden_feats, hidden_feats))\n",
    "        # Output layer\n",
    "        self.layers.append(GINLayer(hidden_feats, out_feats))\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['feature']\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f2aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, g, labels, epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        logits = model(g)\n",
    "        loss = F.cross_entropy(logits[g.ndata['train_mask']], labels[g.ndata['train_mask']])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8583f47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 18.26190185546875\n",
      "Epoch 1, Loss: 18.448213577270508\n",
      "Epoch 2, Loss: 16.28013038635254\n",
      "Epoch 3, Loss: 7.517642498016357\n",
      "Epoch 4, Loss: 0.6931473612785339\n",
      "Epoch 5, Loss: 0.6931473612785339\n",
      "Epoch 6, Loss: 0.6931473612785339\n",
      "Epoch 7, Loss: 0.6931473612785339\n",
      "Epoch 8, Loss: 0.6931473612785339\n",
      "Epoch 9, Loss: 0.6931473612785339\n",
      "Epoch 10, Loss: 0.6931473612785339\n",
      "Epoch 11, Loss: 0.6931473612785339\n",
      "Epoch 12, Loss: 0.6931473612785339\n",
      "Epoch 13, Loss: 0.6931473612785339\n",
      "Epoch 14, Loss: 0.6931473612785339\n",
      "Epoch 15, Loss: 0.6931473612785339\n",
      "Epoch 16, Loss: 0.6931473612785339\n",
      "Epoch 17, Loss: 0.6931473612785339\n",
      "Epoch 18, Loss: 0.6931473612785339\n",
      "Epoch 19, Loss: 0.6931473612785339\n",
      "Epoch 20, Loss: 0.6931473612785339\n",
      "Epoch 21, Loss: 0.6931473612785339\n",
      "Epoch 22, Loss: 0.6931473612785339\n",
      "Epoch 23, Loss: 0.6931473612785339\n",
      "Epoch 24, Loss: 0.6931473612785339\n",
      "Epoch 25, Loss: 0.6931473612785339\n",
      "Epoch 26, Loss: 0.6931473612785339\n",
      "Epoch 27, Loss: 0.6931473612785339\n",
      "Epoch 28, Loss: 0.6931473612785339\n",
      "Epoch 29, Loss: 0.6931473612785339\n",
      "Epoch 30, Loss: 0.6931473612785339\n",
      "Epoch 31, Loss: 0.6931473612785339\n",
      "Epoch 32, Loss: 0.6931473612785339\n",
      "Epoch 33, Loss: 0.6931473612785339\n",
      "Epoch 34, Loss: 0.6931473612785339\n",
      "Epoch 35, Loss: 0.6931473612785339\n",
      "Epoch 36, Loss: 0.6931473612785339\n",
      "Epoch 37, Loss: 0.6931473612785339\n",
      "Epoch 38, Loss: 0.6931473612785339\n",
      "Epoch 39, Loss: 0.6931473612785339\n",
      "Epoch 40, Loss: 0.6931473612785339\n",
      "Epoch 41, Loss: 0.6931473612785339\n",
      "Epoch 42, Loss: 0.6931473612785339\n",
      "Epoch 43, Loss: 0.6931473612785339\n",
      "Epoch 44, Loss: 0.6931473612785339\n",
      "Epoch 45, Loss: 0.6931473612785339\n",
      "Epoch 46, Loss: 0.6931473612785339\n",
      "Epoch 47, Loss: 0.6931473612785339\n",
      "Epoch 48, Loss: 0.6931473612785339\n",
      "Epoch 49, Loss: 0.6931473612785339\n",
      "Epoch 50, Loss: 0.6931473612785339\n",
      "Epoch 51, Loss: 0.6931473612785339\n",
      "Epoch 52, Loss: 0.6931473612785339\n",
      "Epoch 53, Loss: 0.6931473612785339\n",
      "Epoch 54, Loss: 0.6931473612785339\n",
      "Epoch 55, Loss: 0.6931473612785339\n",
      "Epoch 56, Loss: 0.6931473612785339\n",
      "Epoch 57, Loss: 0.6931473612785339\n",
      "Epoch 58, Loss: 0.6931473612785339\n",
      "Epoch 59, Loss: 0.6931473612785339\n",
      "Epoch 60, Loss: 0.6931473612785339\n",
      "Epoch 61, Loss: 0.6931473612785339\n",
      "Epoch 62, Loss: 0.6931473612785339\n",
      "Epoch 63, Loss: 0.6931473612785339\n",
      "Epoch 64, Loss: 0.6931473612785339\n",
      "Epoch 65, Loss: 0.6931473612785339\n",
      "Epoch 66, Loss: 0.6931473612785339\n",
      "Epoch 67, Loss: 0.6931473612785339\n",
      "Epoch 68, Loss: 0.6931473612785339\n",
      "Epoch 69, Loss: 0.6931473612785339\n",
      "Epoch 70, Loss: 0.6931473612785339\n",
      "Epoch 71, Loss: 0.6931473612785339\n",
      "Epoch 72, Loss: 0.6931473612785339\n",
      "Epoch 73, Loss: 0.6931473612785339\n",
      "Epoch 74, Loss: 0.6931473612785339\n",
      "Epoch 75, Loss: 0.6931473612785339\n",
      "Epoch 76, Loss: 0.6931473612785339\n",
      "Epoch 77, Loss: 0.6931473612785339\n",
      "Epoch 78, Loss: 0.6931473612785339\n",
      "Epoch 79, Loss: 0.6931473612785339\n",
      "Epoch 80, Loss: 0.6931473612785339\n",
      "Epoch 81, Loss: 0.6931473612785339\n",
      "Epoch 82, Loss: 0.6931473612785339\n",
      "Epoch 83, Loss: 0.6931473612785339\n",
      "Epoch 84, Loss: 0.6931473612785339\n",
      "Epoch 85, Loss: 0.6931473612785339\n",
      "Epoch 86, Loss: 0.6931473612785339\n",
      "Epoch 87, Loss: 0.6931473612785339\n",
      "Epoch 88, Loss: 0.6931473612785339\n",
      "Epoch 89, Loss: 0.6931473612785339\n",
      "Epoch 90, Loss: 0.6931473612785339\n",
      "Epoch 91, Loss: 0.6931473612785339\n",
      "Epoch 92, Loss: 0.6931473612785339\n",
      "Epoch 93, Loss: 0.6931473612785339\n",
      "Epoch 94, Loss: 0.6931473612785339\n",
      "Epoch 95, Loss: 0.6931473612785339\n",
      "Epoch 96, Loss: 0.6931473612785339\n",
      "Epoch 97, Loss: 0.6931473612785339\n",
      "Epoch 98, Loss: 0.6931473612785339\n",
      "Epoch 99, Loss: 0.6931473612785339\n"
     ]
    }
   ],
   "source": [
    "# Assuming g3 is your graph and it has 'feature' and 'label' as node data\n",
    "in_feats = g3.ndata['feature'].shape[1]\n",
    "hidden_feats = 64  # Example hidden feature size\n",
    "out_feats = len(torch.unique(g3.ndata['label']))  # Number of classes\n",
    "num_layers = 3  # Number of GIN layers\n",
    "\n",
    "model = GINModel(in_feats, hidden_feats, out_feats, num_layers)\n",
    "train(model, g3, g3.ndata['label'], epochs=100, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602a89f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(g3)\n",
    "    test_logits = logits[g3.ndata['test_mask']]\n",
    "    test_labels = g3.ndata['label'][g3.ndata['test_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217dd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ROC-AUC: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming binary classification and logits are output from your model\n",
    "probs = torch.softmax(test_logits, dim=1)[:, 1].numpy()  # Probability for the positive class\n",
    "auc_score = roc_auc_score(test_labels.numpy(), probs)\n",
    "print(f'Test ROC-AUC: {auc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c42c06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
